{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大致統整有4種適用的模型用於預測數據\n",
    "1. 一對多 2. 一對一 3. 多對一 4.多對多\n",
    "\n",
    "一對一\n",
    "model.add(LSTM(10, input_length=shape[1], input_dim=shape[2],return_sequences=True))  \n",
    "model.add(TimeDistributed(Dense(1)))    #可以建這層   or model.add(Dense(1))  \n",
    "  \n",
    "一對多  \n",
    "model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))  \n",
    "model.add(Dense(1))  \n",
    "model.add(RepeatVector(5)) # 增加維度 if input(None,32) output(None,5,32)  \n",
    "  \n",
    "多對一  \n",
    "model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))  \n",
    "model.add(Dense(1))  \n",
    "  \n",
    "多對多  \n",
    "model.add(LSTM(10, input_length=shape[1], input_dim=shape[2], return_sequences=True))  \n",
    "model.add(TimeDistributed(Dense(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# =========================================匯入資料並處理=========================================\n",
    "\n",
    "def readTrain():\n",
    "    train = pd.read_csv(r'D:\\AYA\\test_file\\SPY.csv')\n",
    "    return train\n",
    "\n",
    "def augFeatures(train):\n",
    "    train[\"Date\"] = pd.to_datetime(train[\"Date\"])\n",
    "    train[\"year\"] = train[\"Date\"].dt.year\n",
    "    train[\"month\"] = train[\"Date\"].dt.month\n",
    "    train[\"date\"] = train[\"Date\"].dt.day\n",
    "    train[\"day\"] = train[\"Date\"].dt.dayofweek\n",
    "    return train\n",
    "\n",
    "# 所有數據正規化 (月份、日期也可以)\n",
    "def normalize(train):\n",
    "    train = train.drop([\"Date\"], axis=1)\n",
    "    train_norm = train.apply(lambda x: (x - np.mean(x)) / (np.max(x) - np.min(x)))\n",
    "    return train_norm\n",
    "\n",
    "\n",
    "# 輸入X_train: 利用前30天的Open, High, Low, Close, Adj Close, \n",
    "# Volume, month, year, date, day作為Features，shape為(30, 10)\n",
    "# Y_train: 利用未來5天的Adj Close作為Features，shape為(5,1)\n",
    "\n",
    "def buildTrain(train, pastDay, futureDay):\n",
    "    X_train, Y_train = [], []\n",
    "    for i in range(train.shape[0]-futureDay-pastDay):\n",
    "        X_train.append(np.array(train.iloc[i:i+pastDay]))\n",
    "        Y_train.append(np.array(train.iloc[i+pastDay:i+pastDay+futureDay][\"Adj Close\"]))\n",
    "    return np.array(X_train), np.array(Y_train)\n",
    "\n",
    "# X_train,Y_train = buildTrain(train, 1, 1)\n",
    "\n",
    "# 將資料打散，而非照日期排序\n",
    "def shuffle(X,Y):\n",
    "    np.random.seed(10)\n",
    "    randomList = np.arange(X.shape[0])\n",
    "    np.random.shuffle(randomList)\n",
    "    return X[randomList], Y[randomList]\n",
    "\n",
    "# X,Y = shuffle(X_train,Y_train)\n",
    "\n",
    "# 將Training Data取一部份當作Validation Data\n",
    "def splitData(X,Y,rate):\n",
    "    X_train = X[int(X.shape[0]*rate):]\n",
    "    Y_train = Y[int(Y.shape[0]*rate):]\n",
    "    X_val = X[:int(X.shape[0]*rate)]\n",
    "    Y_val = Y[:int(Y.shape[0]*rate)]\n",
    "    return X_train, Y_train, X_val, Y_val\n",
    "\n",
    "# X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\arthur.chang\\AppData\\Local\\Continuum\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthur.chang\\AppData\\Local\\Continuum\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  import sys\n",
      "C:\\Users\\arthur.chang\\AppData\\Local\\Continuum\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, return_sequences=True, input_shape=(1, 10))`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 10)             840       \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 1, 1)              11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\arthur.chang\\AppData\\Local\\Continuum\\anaconda3\\envs\\mlenv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 5710 samples, validate on 634 samples\n",
      "Epoch 1/1000\n",
      "5710/5710 [==============================] - 1s 147us/step - loss: 0.0719 - val_loss: 0.0531\n",
      "Epoch 2/1000\n",
      "5710/5710 [==============================] - 0s 16us/step - loss: 0.0424 - val_loss: 0.0289\n",
      "Epoch 3/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 0.0203 - val_loss: 0.0114\n",
      "Epoch 4/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 0.0065 - val_loss: 0.0027\n",
      "Epoch 5/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 0.0014 - val_loss: 6.0530e-04\n",
      "Epoch 6/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 4.2884e-04 - val_loss: 3.1422e-04\n",
      "Epoch 7/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 2.8864e-04 - val_loss: 2.4068e-04\n",
      "Epoch 8/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.3588e-04 - val_loss: 2.0360e-04\n",
      "Epoch 9/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.0654e-04 - val_loss: 1.8355e-04\n",
      "Epoch 10/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.8864e-04 - val_loss: 1.7101e-04\n",
      "Epoch 11/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.7582e-04 - val_loss: 1.6136e-04\n",
      "Epoch 12/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.6582e-04 - val_loss: 1.5334e-04\n",
      "Epoch 13/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.5672e-04 - val_loss: 1.4674e-04\n",
      "Epoch 14/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.4865e-04 - val_loss: 1.3967e-04\n",
      "Epoch 15/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.4090e-04 - val_loss: 1.3339e-04\n",
      "Epoch 16/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.3395e-04 - val_loss: 1.2665e-04\n",
      "Epoch 17/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.2677e-04 - val_loss: 1.2053e-04\n",
      "Epoch 18/1000\n",
      "5710/5710 [==============================] - 0s 15us/step - loss: 1.2075e-04 - val_loss: 1.1468e-04\n",
      "Epoch 19/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.1419e-04 - val_loss: 1.0938e-04\n",
      "Epoch 20/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.0798e-04 - val_loss: 1.0386e-04\n",
      "Epoch 21/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 1.0234e-04 - val_loss: 9.8953e-05\n",
      "Epoch 22/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 9.7111e-05 - val_loss: 9.4642e-05\n",
      "Epoch 23/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 9.2219e-05 - val_loss: 9.0218e-05\n",
      "Epoch 24/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 8.7503e-05 - val_loss: 8.5775e-05\n",
      "Epoch 25/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 8.3404e-05 - val_loss: 8.1884e-05\n",
      "Epoch 26/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 7.9312e-05 - val_loss: 7.8411e-05\n",
      "Epoch 27/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 7.5530e-05 - val_loss: 7.5136e-05\n",
      "Epoch 28/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 7.2285e-05 - val_loss: 7.2074e-05\n",
      "Epoch 29/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 6.9088e-05 - val_loss: 6.9319e-05\n",
      "Epoch 30/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 6.6413e-05 - val_loss: 6.6783e-05\n",
      "Epoch 31/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 6.3537e-05 - val_loss: 6.4494e-05\n",
      "Epoch 32/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 6.1279e-05 - val_loss: 6.2627e-05\n",
      "Epoch 33/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.9179e-05 - val_loss: 6.0628e-05\n",
      "Epoch 34/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.7383e-05 - val_loss: 5.8997e-05\n",
      "Epoch 35/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.5567e-05 - val_loss: 5.7263e-05\n",
      "Epoch 36/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.4015e-05 - val_loss: 5.5979e-05\n",
      "Epoch 37/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 5.2532e-05 - val_loss: 5.4813e-05\n",
      "Epoch 38/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.1494e-05 - val_loss: 5.3662e-05\n",
      "Epoch 39/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 5.0349e-05 - val_loss: 5.2821e-05\n",
      "Epoch 40/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.9290e-05 - val_loss: 5.2413e-05\n",
      "Epoch 41/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.8661e-05 - val_loss: 5.1400e-05\n",
      "Epoch 42/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.7682e-05 - val_loss: 5.0462e-05\n",
      "Epoch 43/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.6848e-05 - val_loss: 5.0115e-05\n",
      "Epoch 44/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.6343e-05 - val_loss: 4.9396e-05\n",
      "Epoch 45/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.5614e-05 - val_loss: 4.8471e-05\n",
      "Epoch 46/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.5048e-05 - val_loss: 4.7801e-05\n",
      "Epoch 47/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.4579e-05 - val_loss: 4.7365e-05\n",
      "Epoch 48/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.4141e-05 - val_loss: 4.6966e-05\n",
      "Epoch 49/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.3714e-05 - val_loss: 4.6305e-05\n",
      "Epoch 50/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.3173e-05 - val_loss: 4.6287e-05\n",
      "Epoch 51/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.2919e-05 - val_loss: 4.5762e-05\n",
      "Epoch 52/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.2360e-05 - val_loss: 4.5142e-05\n",
      "Epoch 53/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.1949e-05 - val_loss: 4.4446e-05\n",
      "Epoch 54/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.1618e-05 - val_loss: 4.4335e-05\n",
      "Epoch 55/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.1268e-05 - val_loss: 4.4022e-05\n",
      "Epoch 56/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.0865e-05 - val_loss: 4.3590e-05\n",
      "Epoch 57/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 4.0699e-05 - val_loss: 4.3241e-05\n",
      "Epoch 58/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 4.0236e-05 - val_loss: 4.2993e-05\n",
      "Epoch 59/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.9938e-05 - val_loss: 4.2070e-05\n",
      "Epoch 60/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.9672e-05 - val_loss: 4.1968e-05\n",
      "Epoch 61/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.9241e-05 - val_loss: 4.1508e-05\n",
      "Epoch 62/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.9257e-05 - val_loss: 4.1508e-05\n",
      "Epoch 63/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.8531e-05 - val_loss: 4.0708e-05\n",
      "Epoch 64/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.8480e-05 - val_loss: 4.0614e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/1000\n",
      "5710/5710 [==============================] - 0s 11us/step - loss: 3.7907e-05 - val_loss: 4.0310e-05\n",
      "Epoch 66/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.7818e-05 - val_loss: 3.9704e-05\n",
      "Epoch 67/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.7455e-05 - val_loss: 3.9387e-05\n",
      "Epoch 68/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.7098e-05 - val_loss: 3.9526e-05\n",
      "Epoch 69/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.6917e-05 - val_loss: 3.8501e-05\n",
      "Epoch 70/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.6594e-05 - val_loss: 3.8037e-05\n",
      "Epoch 71/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.6166e-05 - val_loss: 3.8876e-05\n",
      "Epoch 72/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.6150e-05 - val_loss: 3.7583e-05\n",
      "Epoch 73/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.5710e-05 - val_loss: 3.7289e-05\n",
      "Epoch 74/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.5103e-05 - val_loss: 3.7114e-05\n",
      "Epoch 75/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.4942e-05 - val_loss: 3.6409e-05\n",
      "Epoch 76/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.5139e-05 - val_loss: 3.6548e-05\n",
      "Epoch 77/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.4396e-05 - val_loss: 3.6546e-05\n",
      "Epoch 78/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.3955e-05 - val_loss: 3.5518e-05\n",
      "Epoch 79/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.3783e-05 - val_loss: 3.5214e-05\n",
      "Epoch 80/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.3419e-05 - val_loss: 3.4819e-05\n",
      "Epoch 81/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.3386e-05 - val_loss: 3.5279e-05\n",
      "Epoch 82/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.2912e-05 - val_loss: 3.4302e-05\n",
      "Epoch 83/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.2660e-05 - val_loss: 3.4478e-05\n",
      "Epoch 84/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.2662e-05 - val_loss: 3.3394e-05\n",
      "Epoch 85/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.2080e-05 - val_loss: 3.3826e-05\n",
      "Epoch 86/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.1983e-05 - val_loss: 3.2789e-05\n",
      "Epoch 87/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.1766e-05 - val_loss: 3.2921e-05\n",
      "Epoch 88/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.1549e-05 - val_loss: 3.2692e-05\n",
      "Epoch 89/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.1384e-05 - val_loss: 3.1792e-05\n",
      "Epoch 90/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.0863e-05 - val_loss: 3.2224e-05\n",
      "Epoch 91/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.0948e-05 - val_loss: 3.1927e-05\n",
      "Epoch 92/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.0305e-05 - val_loss: 3.1413e-05\n",
      "Epoch 93/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.0169e-05 - val_loss: 3.0840e-05\n",
      "Epoch 94/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 3.0569e-05 - val_loss: 3.1289e-05\n",
      "Epoch 95/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.9686e-05 - val_loss: 3.0968e-05\n",
      "Epoch 96/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.9390e-05 - val_loss: 2.9989e-05\n",
      "Epoch 97/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.9317e-05 - val_loss: 2.9821e-05\n",
      "Epoch 98/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.8950e-05 - val_loss: 2.9450e-05\n",
      "Epoch 99/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.9085e-05 - val_loss: 2.9411e-05\n",
      "Epoch 100/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.8634e-05 - val_loss: 2.9321e-05\n",
      "Epoch 101/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.8039e-05 - val_loss: 2.8943e-05\n",
      "Epoch 102/1000\n",
      "5710/5710 [==============================] - ETA: 0s - loss: 2.8365e-0 - 0s 13us/step - loss: 2.7998e-05 - val_loss: 2.8755e-05\n",
      "Epoch 103/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.8022e-05 - val_loss: 2.8659e-05\n",
      "Epoch 104/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.7697e-05 - val_loss: 2.7939e-05\n",
      "Epoch 105/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.7630e-05 - val_loss: 2.7868e-05\n",
      "Epoch 106/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.7429e-05 - val_loss: 2.7568e-05\n",
      "Epoch 107/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.7292e-05 - val_loss: 2.8209e-05\n",
      "Epoch 108/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.7778e-05 - val_loss: 2.7028e-05\n",
      "Epoch 109/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.7065e-05 - val_loss: 2.7494e-05\n",
      "Epoch 110/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.6693e-05 - val_loss: 2.7262e-05\n",
      "Epoch 111/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.6448e-05 - val_loss: 2.6535e-05\n",
      "Epoch 112/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.6628e-05 - val_loss: 2.7967e-05\n",
      "Epoch 113/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.7148e-05 - val_loss: 2.6152e-05\n",
      "Epoch 114/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.6184e-05 - val_loss: 2.6910e-05\n",
      "Epoch 115/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.6342e-05 - val_loss: 2.6202e-05\n",
      "Epoch 116/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5659e-05 - val_loss: 2.5694e-05\n",
      "Epoch 117/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5597e-05 - val_loss: 2.5586e-05\n",
      "Epoch 118/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5426e-05 - val_loss: 2.5148e-05\n",
      "Epoch 119/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5351e-05 - val_loss: 2.5912e-05\n",
      "Epoch 120/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5630e-05 - val_loss: 2.6069e-05\n",
      "Epoch 121/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5602e-05 - val_loss: 2.5815e-05\n",
      "Epoch 122/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5696e-05 - val_loss: 2.6008e-05\n",
      "Epoch 123/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5377e-05 - val_loss: 2.4719e-05\n",
      "Epoch 124/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4859e-05 - val_loss: 2.5356e-05\n",
      "Epoch 125/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5650e-05 - val_loss: 2.4502e-05\n",
      "Epoch 126/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4981e-05 - val_loss: 2.5406e-05\n",
      "Epoch 127/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4628e-05 - val_loss: 2.4853e-05\n",
      "Epoch 128/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.5238e-05 - val_loss: 2.4220e-05\n",
      "Epoch 129/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4541e-05 - val_loss: 2.4397e-05\n",
      "Epoch 130/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4772e-05 - val_loss: 2.5878e-05\n",
      "Epoch 131/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4650e-05 - val_loss: 2.4312e-05\n",
      "Epoch 132/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4501e-05 - val_loss: 2.3982e-05\n",
      "Epoch 133/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4577e-05 - val_loss: 2.3805e-05\n",
      "Epoch 134/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4414e-05 - val_loss: 2.3806e-05\n",
      "Epoch 135/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4311e-05 - val_loss: 2.4056e-05\n",
      "Epoch 136/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4263e-05 - val_loss: 2.3836e-05\n",
      "Epoch 137/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4627e-05 - val_loss: 2.3329e-05\n",
      "Epoch 138/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4472e-05 - val_loss: 2.3679e-05\n",
      "Epoch 139/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4230e-05 - val_loss: 2.3727e-05\n",
      "Epoch 140/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4269e-05 - val_loss: 2.3802e-05\n",
      "Epoch 141/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4377e-05 - val_loss: 2.3633e-05\n",
      "Epoch 142/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4293e-05 - val_loss: 2.3907e-05\n",
      "Epoch 143/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3893e-05 - val_loss: 2.3486e-05\n",
      "Epoch 144/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3791e-05 - val_loss: 2.2953e-05\n",
      "Epoch 145/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3894e-05 - val_loss: 2.3008e-05\n",
      "Epoch 146/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3854e-05 - val_loss: 2.3410e-05\n",
      "Epoch 147/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4118e-05 - val_loss: 2.2634e-05\n",
      "Epoch 148/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4240e-05 - val_loss: 2.3139e-05\n",
      "Epoch 149/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3688e-05 - val_loss: 2.3897e-05\n",
      "Epoch 150/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3842e-05 - val_loss: 2.3323e-05\n",
      "Epoch 151/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3534e-05 - val_loss: 2.3008e-05\n",
      "Epoch 152/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3961e-05 - val_loss: 2.2659e-05\n",
      "Epoch 153/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3478e-05 - val_loss: 2.2912e-05\n",
      "Epoch 154/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3459e-05 - val_loss: 2.3223e-05\n",
      "Epoch 155/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3909e-05 - val_loss: 2.8444e-05\n",
      "Epoch 156/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4404e-05 - val_loss: 2.2976e-05\n",
      "Epoch 157/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3838e-05 - val_loss: 2.3768e-05\n",
      "Epoch 158/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4015e-05 - val_loss: 2.3974e-05\n",
      "Epoch 159/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3549e-05 - val_loss: 2.2384e-05\n",
      "Epoch 160/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3579e-05 - val_loss: 2.3763e-05\n",
      "Epoch 161/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.3879e-05 - val_loss: 2.4811e-05\n",
      "Epoch 162/1000\n",
      "5710/5710 [==============================] - 0s 14us/step - loss: 2.3937e-05 - val_loss: 2.2743e-05\n",
      "Epoch 163/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3355e-05 - val_loss: 2.2707e-05\n",
      "Epoch 164/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3748e-05 - val_loss: 2.3633e-05\n",
      "Epoch 165/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3659e-05 - val_loss: 2.3133e-05\n",
      "Epoch 166/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3272e-05 - val_loss: 2.2220e-05\n",
      "Epoch 167/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3690e-05 - val_loss: 2.1987e-05\n",
      "Epoch 168/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3345e-05 - val_loss: 2.3847e-05\n",
      "Epoch 169/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3879e-05 - val_loss: 2.6213e-05\n",
      "Epoch 170/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4277e-05 - val_loss: 2.3717e-05\n",
      "Epoch 171/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3882e-05 - val_loss: 2.2365e-05\n",
      "Epoch 172/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3066e-05 - val_loss: 2.2174e-05\n",
      "Epoch 173/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4342e-05 - val_loss: 2.2940e-05\n",
      "Epoch 174/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3533e-05 - val_loss: 2.2467e-05\n",
      "Epoch 175/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3430e-05 - val_loss: 2.6543e-05\n",
      "Epoch 176/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.4050e-05 - val_loss: 2.2647e-05\n",
      "Epoch 177/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3722e-05 - val_loss: 2.2363e-05\n",
      "Epoch 178/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3095e-05 - val_loss: 2.3362e-05\n",
      "Epoch 179/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3874e-05 - val_loss: 2.2616e-05\n",
      "Epoch 180/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3311e-05 - val_loss: 2.2802e-05\n",
      "Epoch 181/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3445e-05 - val_loss: 2.3062e-05\n",
      "Epoch 182/1000\n",
      "5710/5710 [==============================] - 0s 13us/step - loss: 2.3895e-05 - val_loss: 2.2042e-05\n",
      "Epoch 00182: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23beb415828>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================================建模型=========================================\n",
    "\n",
    "# 一對一的模型 因此return_sequences 也可設為False ，\n",
    "# 但Y_train 以及Y_val的維度需改為二維(5710,1)以及(634,1)\n",
    "def buildOneToOneModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_length=shape[1], input_dim=shape[2],return_sequences=True))\n",
    "    # output shape: (1, 1)\n",
    "    model.add(TimeDistributed(Dense(1)))    # or use model.add(Dense(1)) # 多對一的模型不能建這層\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# 將過去的天數pastDay設為1，預測的天數futureDay也設為1\n",
    "\n",
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 1, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,np.newaxis]\n",
    "Y_val = Y_val[:,np.newaxis]\n",
    "\n",
    "model = buildOneToOneModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthur.chang\\AppData\\Local\\Continuum\\anaconda2\\envs\\Python3.6\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "C:\\Users\\arthur.chang\\AppData\\Local\\Continuum\\anaconda2\\envs\\Python3.6\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, input_shape=(30, 10))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5684 samples, validate on 631 samples\n",
      "Epoch 1/1000\n",
      "5684/5684 [==============================] - 2s 288us/step - loss: 0.1267 - val_loss: 0.0494\n",
      "Epoch 2/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 0.0200 - val_loss: 0.0066\n",
      "Epoch 3/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 0.0031 - val_loss: 0.0015\n",
      "Epoch 4/1000\n",
      "5684/5684 [==============================] - 1s 88us/step - loss: 0.0011 - val_loss: 8.6022e-04\n",
      "Epoch 5/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 6.9926e-04 - val_loss: 5.9268e-04\n",
      "Epoch 6/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 5.1185e-04 - val_loss: 4.5970e-04\n",
      "Epoch 7/1000\n",
      "5684/5684 [==============================] - 0s 86us/step - loss: 4.0483e-04 - val_loss: 3.7455e-04\n",
      "Epoch 8/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 3.3369e-04 - val_loss: 3.1473e-04\n",
      "Epoch 9/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 2.8185e-04 - val_loss: 2.7229e-04\n",
      "Epoch 10/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 2.4456e-04 - val_loss: 2.3513e-04\n",
      "Epoch 11/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 2.1472e-04 - val_loss: 2.0945e-04\n",
      "Epoch 12/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 1.9198e-04 - val_loss: 1.8791e-04\n",
      "Epoch 13/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 1.7221e-04 - val_loss: 1.7390e-04\n",
      "Epoch 14/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 1.5686e-04 - val_loss: 1.5775e-04\n",
      "Epoch 15/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 1.4253e-04 - val_loss: 1.4323e-04\n",
      "Epoch 16/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 1.3047e-04 - val_loss: 1.3234e-04\n",
      "Epoch 17/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 1.2085e-04 - val_loss: 1.2284e-04\n",
      "Epoch 18/1000\n",
      "5684/5684 [==============================] - 0s 86us/step - loss: 1.1217e-04 - val_loss: 1.1464e-04\n",
      "Epoch 19/1000\n",
      "5684/5684 [==============================] - 1s 88us/step - loss: 1.0520e-04 - val_loss: 1.0722e-04\n",
      "Epoch 20/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 9.8960e-05 - val_loss: 1.0216e-04\n",
      "Epoch 21/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 9.2637e-05 - val_loss: 9.6600e-05\n",
      "Epoch 22/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 8.7078e-05 - val_loss: 9.1063e-05\n",
      "Epoch 23/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 8.2614e-05 - val_loss: 8.6298e-05\n",
      "Epoch 24/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 7.9054e-05 - val_loss: 8.6300e-05\n",
      "Epoch 25/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 7.5487e-05 - val_loss: 7.9105e-05\n",
      "Epoch 26/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 7.1624e-05 - val_loss: 7.5933e-05\n",
      "Epoch 27/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 6.9267e-05 - val_loss: 7.3358e-05\n",
      "Epoch 28/1000\n",
      "5684/5684 [==============================] - 1s 88us/step - loss: 6.6549e-05 - val_loss: 7.1327e-05\n",
      "Epoch 29/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 6.5647e-05 - val_loss: 7.0246e-05\n",
      "Epoch 30/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 6.3249e-05 - val_loss: 6.6972e-05\n",
      "Epoch 31/1000\n",
      "5684/5684 [==============================] - 1s 94us/step - loss: 6.2139e-05 - val_loss: 6.5873e-05\n",
      "Epoch 32/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 5.9880e-05 - val_loss: 6.4572e-05\n",
      "Epoch 33/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 5.8440e-05 - val_loss: 6.2329e-05\n",
      "Epoch 34/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 5.7214e-05 - val_loss: 6.1963e-05\n",
      "Epoch 35/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 5.6918e-05 - val_loss: 6.1414e-05\n",
      "Epoch 36/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 5.5150e-05 - val_loss: 6.0726e-05\n",
      "Epoch 37/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 5.4686e-05 - val_loss: 5.9221e-05\n",
      "Epoch 38/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 5.3455e-05 - val_loss: 5.7647e-05\n",
      "Epoch 39/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 5.2869e-05 - val_loss: 5.8908e-05\n",
      "Epoch 40/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 5.2990e-05 - val_loss: 5.6377e-05\n",
      "Epoch 41/1000\n",
      "5684/5684 [==============================] - 0s 87us/step - loss: 5.1255e-05 - val_loss: 5.6738e-05\n",
      "Epoch 42/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 5.1379e-05 - val_loss: 5.5274e-05\n",
      "Epoch 43/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 5.0934e-05 - val_loss: 5.4823e-05\n",
      "Epoch 44/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 5.0419e-05 - val_loss: 5.3925e-05\n",
      "Epoch 45/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 4.9816e-05 - val_loss: 5.4725e-05\n",
      "Epoch 46/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 4.9568e-05 - val_loss: 5.3687e-05\n",
      "Epoch 47/1000\n",
      "5684/5684 [==============================] - 1s 88us/step - loss: 4.8891e-05 - val_loss: 5.2673e-05\n",
      "Epoch 48/1000\n",
      "5684/5684 [==============================] - 1s 88us/step - loss: 4.7446e-05 - val_loss: 5.1837e-05\n",
      "Epoch 49/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 4.7029e-05 - val_loss: 5.1115e-05\n",
      "Epoch 50/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 4.6999e-05 - val_loss: 5.0953e-05\n",
      "Epoch 51/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 4.6342e-05 - val_loss: 5.1913e-05\n",
      "Epoch 52/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 4.5586e-05 - val_loss: 5.0014e-05\n",
      "Epoch 53/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 4.5534e-05 - val_loss: 4.9980e-05\n",
      "Epoch 54/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 4.5139e-05 - val_loss: 4.9801e-05\n",
      "Epoch 55/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 4.5137e-05 - val_loss: 5.0685e-05\n",
      "Epoch 56/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 4.4360e-05 - val_loss: 4.9633e-05\n",
      "Epoch 57/1000\n",
      "5684/5684 [==============================] - 1s 88us/step - loss: 4.4454e-05 - val_loss: 4.9792e-05\n",
      "Epoch 58/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 4.4574e-05 - val_loss: 4.8023e-05\n",
      "Epoch 59/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 4.3658e-05 - val_loss: 4.6650e-05\n",
      "Epoch 60/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 4.2937e-05 - val_loss: 4.6881e-05\n",
      "Epoch 61/1000\n",
      "5684/5684 [==============================] - 0s 88us/step - loss: 4.2493e-05 - val_loss: 4.6813e-05\n",
      "Epoch 62/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 4.2298e-05 - val_loss: 4.8569e-05\n",
      "Epoch 63/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 4.2115e-05 - val_loss: 4.8873e-05\n",
      "Epoch 64/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 4.2606e-05 - val_loss: 4.7138e-05\n",
      "Epoch 65/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 4.1324e-05 - val_loss: 4.5371e-05\n",
      "Epoch 66/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 4.1765e-05 - val_loss: 4.7999e-05\n",
      "Epoch 67/1000\n",
      "5684/5684 [==============================] - 1s 93us/step - loss: 4.0876e-05 - val_loss: 4.4736e-05\n",
      "Epoch 68/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 4.1676e-05 - val_loss: 4.5295e-05\n",
      "Epoch 69/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 4.0147e-05 - val_loss: 4.4610e-05\n",
      "Epoch 70/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 4.0102e-05 - val_loss: 4.3773e-05\n",
      "Epoch 71/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.9095e-05 - val_loss: 4.3482e-05\n",
      "Epoch 72/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.9262e-05 - val_loss: 4.4986e-05\n",
      "Epoch 73/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.9667e-05 - val_loss: 4.2959e-05\n",
      "Epoch 74/1000\n",
      "5684/5684 [==============================] - 1s 89us/step - loss: 3.9602e-05 - val_loss: 4.4511e-05\n",
      "Epoch 75/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 3.8834e-05 - val_loss: 4.4975e-05\n",
      "Epoch 76/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.8793e-05 - val_loss: 4.2760e-05\n",
      "Epoch 77/1000\n",
      "5684/5684 [==============================] - 1s 90us/step - loss: 3.9101e-05 - val_loss: 4.5065e-05\n",
      "Epoch 78/1000\n",
      "5684/5684 [==============================] - 1s 91us/step - loss: 3.7510e-05 - val_loss: 4.1646e-05\n",
      "Epoch 79/1000\n",
      "5684/5684 [==============================] - 1s 92us/step - loss: 3.7944e-05 - val_loss: 4.1896e-05\n",
      "Epoch 80/1000\n",
      "5684/5684 [==============================] - 1s 96us/step - loss: 3.7938e-05 - val_loss: 4.1593e-05\n",
      "Epoch 81/1000\n",
      "5684/5684 [==============================] - 1s 102us/step - loss: 3.7879e-05 - val_loss: 4.1600e-05\n",
      "Epoch 82/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 3.7196e-05 - val_loss: 4.2224e-05\n",
      "Epoch 83/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.7252e-05 - val_loss: 4.0992e-05\n",
      "Epoch 84/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 3.6904e-05 - val_loss: 4.7869e-05\n",
      "Epoch 85/1000\n",
      "5684/5684 [==============================] - 1s 95us/step - loss: 3.7401e-05 - val_loss: 4.5916e-05\n",
      "Epoch 86/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 3.6671e-05 - val_loss: 3.9907e-05\n",
      "Epoch 87/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 3.7218e-05 - val_loss: 4.0431e-05\n",
      "Epoch 88/1000\n",
      "5684/5684 [==============================] - 1s 98us/step - loss: 3.5665e-05 - val_loss: 4.0541e-05\n",
      "Epoch 89/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 3.5434e-05 - val_loss: 3.8951e-05\n",
      "Epoch 90/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 3.5372e-05 - val_loss: 3.9874e-05\n",
      "Epoch 91/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 3.6676e-05 - val_loss: 3.8875e-05\n",
      "Epoch 92/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 3.5900e-05 - val_loss: 4.0786e-05\n",
      "Epoch 93/1000\n",
      "5684/5684 [==============================] - 1s 99us/step - loss: 3.5326e-05 - val_loss: 3.9874e-05\n",
      "Epoch 94/1000\n",
      "5684/5684 [==============================] - 1s 100us/step - loss: 3.4879e-05 - val_loss: 3.7805e-05\n",
      "Epoch 95/1000\n",
      "5684/5684 [==============================] - 1s 97us/step - loss: 3.4588e-05 - val_loss: 3.7859e-05\n",
      "Epoch 96/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 3.4448e-05 - val_loss: 3.8045e-05\n",
      "Epoch 97/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 3.4797e-05 - val_loss: 3.9837e-05\n",
      "Epoch 98/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 3.4261e-05 - val_loss: 3.9730e-05\n",
      "Epoch 99/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 3.4601e-05 - val_loss: 3.8742e-05\n",
      "Epoch 100/1000\n",
      "5684/5684 [==============================] - 1s 117us/step - loss: 3.3879e-05 - val_loss: 3.7122e-05\n",
      "Epoch 101/1000\n",
      "5684/5684 [==============================] - 1s 121us/step - loss: 3.3105e-05 - val_loss: 3.6517e-05\n",
      "Epoch 102/1000\n",
      "5684/5684 [==============================] - 1s 117us/step - loss: 3.3473e-05 - val_loss: 3.6802e-05\n",
      "Epoch 103/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 3.2862e-05 - val_loss: 3.6629e-05\n",
      "Epoch 104/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 3.3498e-05 - val_loss: 4.2864e-05\n",
      "Epoch 105/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 3.4599e-05 - val_loss: 3.6525e-05\n",
      "Epoch 106/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 3.2375e-05 - val_loss: 3.5790e-05\n",
      "Epoch 107/1000\n",
      "5684/5684 [==============================] - 1s 109us/step - loss: 3.4702e-05 - val_loss: 3.8537e-05\n",
      "Epoch 108/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 3.2230e-05 - val_loss: 3.8609e-05\n",
      "Epoch 109/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 3.2931e-05 - val_loss: 3.5856e-05\n",
      "Epoch 110/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 3.2016e-05 - val_loss: 3.5013e-05\n",
      "Epoch 111/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 3.2390e-05 - val_loss: 4.1461e-05\n",
      "Epoch 112/1000\n",
      "5684/5684 [==============================] - 1s 109us/step - loss: 3.2009e-05 - val_loss: 3.3908e-05\n",
      "Epoch 113/1000\n",
      "5684/5684 [==============================] - 1s 112us/step - loss: 3.2185e-05 - val_loss: 3.4445e-05\n",
      "Epoch 114/1000\n",
      "5684/5684 [==============================] - 1s 117us/step - loss: 3.3177e-05 - val_loss: 3.8192e-05\n",
      "Epoch 115/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 3.2961e-05 - val_loss: 3.4045e-05\n",
      "Epoch 116/1000\n",
      "5684/5684 [==============================] - 1s 112us/step - loss: 3.2540e-05 - val_loss: 3.6245e-05\n",
      "Epoch 117/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 3.1917e-05 - val_loss: 3.9871e-05\n",
      "Epoch 118/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 3.2211e-05 - val_loss: 3.5403e-05\n",
      "Epoch 119/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 3.1054e-05 - val_loss: 3.6396e-05\n",
      "Epoch 120/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 3.0863e-05 - val_loss: 3.4796e-05\n",
      "Epoch 121/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 3.2388e-05 - val_loss: 3.8604e-05\n",
      "Epoch 122/1000\n",
      "5684/5684 [==============================] - 1s 110us/step - loss: 3.0495e-05 - val_loss: 3.4747e-05\n",
      "Epoch 123/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 3.1199e-05 - val_loss: 3.2741e-05\n",
      "Epoch 124/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 3.1110e-05 - val_loss: 3.4594e-05\n",
      "Epoch 125/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 3.1061e-05 - val_loss: 3.3483e-05\n",
      "Epoch 126/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 3.2165e-05 - val_loss: 3.4769e-05\n",
      "Epoch 127/1000\n",
      "5684/5684 [==============================] - 1s 135us/step - loss: 3.0698e-05 - val_loss: 3.7798e-05\n",
      "Epoch 128/1000\n",
      "5684/5684 [==============================] - 1s 130us/step - loss: 3.0034e-05 - val_loss: 3.8135e-05\n",
      "Epoch 129/1000\n",
      "5684/5684 [==============================] - 1s 112us/step - loss: 3.0113e-05 - val_loss: 3.2843e-05\n",
      "Epoch 130/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.9751e-05 - val_loss: 3.3747e-05\n",
      "Epoch 131/1000\n",
      "5684/5684 [==============================] - 1s 126us/step - loss: 2.9639e-05 - val_loss: 3.2263e-05\n",
      "Epoch 132/1000\n",
      "5684/5684 [==============================] - 1s 138us/step - loss: 2.9248e-05 - val_loss: 3.2181e-05\n",
      "Epoch 133/1000\n",
      "5684/5684 [==============================] - 1s 134us/step - loss: 3.0667e-05 - val_loss: 3.5031e-05\n",
      "Epoch 134/1000\n",
      "5684/5684 [==============================] - 1s 125us/step - loss: 2.9474e-05 - val_loss: 3.1322e-05\n",
      "Epoch 135/1000\n",
      "5684/5684 [==============================] - 1s 140us/step - loss: 2.8562e-05 - val_loss: 3.1458e-05\n",
      "Epoch 136/1000\n",
      "5684/5684 [==============================] - 1s 130us/step - loss: 3.0317e-05 - val_loss: 3.4507e-05\n",
      "Epoch 137/1000\n",
      "5684/5684 [==============================] - 1s 115us/step - loss: 2.9798e-05 - val_loss: 3.2635e-05\n",
      "Epoch 138/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.9567e-05 - val_loss: 3.1751e-05\n",
      "Epoch 139/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 2.8920e-05 - val_loss: 3.6001e-05\n",
      "Epoch 140/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 2.8463e-05 - val_loss: 3.4419e-05\n",
      "Epoch 141/1000\n",
      "5684/5684 [==============================] - 1s 111us/step - loss: 2.8349e-05 - val_loss: 3.0679e-05\n",
      "Epoch 142/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 2.9035e-05 - val_loss: 3.2463e-05\n",
      "Epoch 143/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 2.8574e-05 - val_loss: 3.0498e-05\n",
      "Epoch 144/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 2.8515e-05 - val_loss: 3.4697e-05\n",
      "Epoch 145/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.8572e-05 - val_loss: 3.0497e-05\n",
      "Epoch 146/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.8340e-05 - val_loss: 3.4860e-05\n",
      "Epoch 147/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 3.0044e-05 - val_loss: 5.0792e-05\n",
      "Epoch 148/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 2.9133e-05 - val_loss: 2.9910e-05\n",
      "Epoch 149/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 2.8057e-05 - val_loss: 3.0888e-05\n",
      "Epoch 150/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 3.0912e-05 - val_loss: 3.1103e-05\n",
      "Epoch 151/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.8084e-05 - val_loss: 3.3537e-05\n",
      "Epoch 152/1000\n",
      "5684/5684 [==============================] - 1s 104us/step - loss: 2.7832e-05 - val_loss: 3.0822e-05\n",
      "Epoch 153/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 2.6868e-05 - val_loss: 2.9411e-05\n",
      "Epoch 154/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 2.9325e-05 - val_loss: 3.3635e-05\n",
      "Epoch 155/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 2.9215e-05 - val_loss: 2.9854e-05\n",
      "Epoch 156/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 3.0978e-05 - val_loss: 3.0367e-05\n",
      "Epoch 157/1000\n",
      "5684/5684 [==============================] - 1s 105us/step - loss: 2.7200e-05 - val_loss: 3.0738e-05\n",
      "Epoch 158/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 2.9078e-05 - val_loss: 3.5356e-05\n",
      "Epoch 159/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.7982e-05 - val_loss: 3.0417e-05\n",
      "Epoch 160/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.6590e-05 - val_loss: 3.1527e-05\n",
      "Epoch 161/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 2.6416e-05 - val_loss: 2.8626e-05\n",
      "Epoch 162/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.7223e-05 - val_loss: 2.9884e-05\n",
      "Epoch 163/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.6993e-05 - val_loss: 3.1995e-05\n",
      "Epoch 164/1000\n",
      "5684/5684 [==============================] - 1s 106us/step - loss: 2.7206e-05 - val_loss: 3.0188e-05\n",
      "Epoch 165/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 2.7257e-05 - val_loss: 2.8624e-05\n",
      "Epoch 166/1000\n",
      "5684/5684 [==============================] - 1s 109us/step - loss: 2.8498e-05 - val_loss: 3.0290e-05\n",
      "Epoch 167/1000\n",
      "5684/5684 [==============================] - 1s 107us/step - loss: 2.6441e-05 - val_loss: 3.1131e-05\n",
      "Epoch 168/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 2.8864e-05 - val_loss: 3.4114e-05\n",
      "Epoch 169/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 2.8441e-05 - val_loss: 3.2461e-05\n",
      "Epoch 170/1000\n",
      "5684/5684 [==============================] - 1s 103us/step - loss: 2.7583e-05 - val_loss: 2.9145e-05\n",
      "Epoch 171/1000\n",
      "5684/5684 [==============================] - 1s 108us/step - loss: 2.7000e-05 - val_loss: 3.1568e-05\n",
      "Epoch 00171: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1caf72e3eb8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多對一模型\n",
    "# LSTM參數return_sequences=False，未設定時default也為False，\n",
    "# 而且不可使用TimeDistribution\n",
    "\n",
    "def buildManyToOneModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))\n",
    "    # output shape: (1, 1)\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model\n",
    "# 需要設定的有pastDay=30、future=1 ，且注意Y_train 的維度需為二維\n",
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 30, 1)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "# because no return sequence, Y_train and Y_val shape must be 2 dimension\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "model = buildManyToOneModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "# https://medium.com/ai%E5%8F%8D%E6%96%97%E5%9F%8E/learning-model-earlystopping%E4%BB%8B%E7%B4%B9-%E8%BD%89%E9%8C%84-f364f4f220fb\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthur.chang\\AppData\\Local\\Continuum\\anaconda2\\envs\\Python3.6\\lib\\site-packages\\ipykernel\\__main__.py:6: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "C:\\Users\\arthur.chang\\AppData\\Local\\Continuum\\anaconda2\\envs\\Python3.6\\lib\\site-packages\\ipykernel\\__main__.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, input_shape=(1, 10))`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 5, 1)              0         \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5706 samples, validate on 634 samples\n",
      "Epoch 1/1000\n",
      "5706/5706 [==============================] - 1s 189us/step - loss: 0.0716 - val_loss: 0.0550\n",
      "Epoch 2/1000\n",
      "5706/5706 [==============================] - 0s 14us/step - loss: 0.0420 - val_loss: 0.0293\n",
      "Epoch 3/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 0.0200 - val_loss: 0.0113\n",
      "Epoch 4/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 0.0064 - val_loss: 0.0027\n",
      "Epoch 5/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 0.0014 - val_loss: 6.1331e-04\n",
      "Epoch 6/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 4.6460e-04 - val_loss: 3.6029e-04\n",
      "Epoch 7/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 3.3049e-04 - val_loss: 2.9071e-04\n",
      "Epoch 8/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 2.7846e-04 - val_loss: 2.5401e-04\n",
      "Epoch 9/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 2.4893e-04 - val_loss: 2.3405e-04\n",
      "Epoch 10/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 2.3082e-04 - val_loss: 2.1897e-04\n",
      "Epoch 11/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 2.1782e-04 - val_loss: 2.0777e-04\n",
      "Epoch 12/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 2.0683e-04 - val_loss: 1.9822e-04\n",
      "Epoch 13/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.9747e-04 - val_loss: 1.9014e-04\n",
      "Epoch 14/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.8893e-04 - val_loss: 1.8215e-04\n",
      "Epoch 15/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.8074e-04 - val_loss: 1.7541e-04\n",
      "Epoch 16/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.7348e-04 - val_loss: 1.6824e-04\n",
      "Epoch 17/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.6639e-04 - val_loss: 1.6206e-04\n",
      "Epoch 18/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.5909e-04 - val_loss: 1.5522e-04\n",
      "Epoch 19/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.5244e-04 - val_loss: 1.4927e-04\n",
      "Epoch 20/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.4636e-04 - val_loss: 1.4353e-04\n",
      "Epoch 21/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.4045e-04 - val_loss: 1.3833e-04\n",
      "Epoch 22/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.3492e-04 - val_loss: 1.3208e-04\n",
      "Epoch 23/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.2967e-04 - val_loss: 1.2783e-04\n",
      "Epoch 24/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.2481e-04 - val_loss: 1.2256e-04\n",
      "Epoch 25/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.2061e-04 - val_loss: 1.1859e-04\n",
      "Epoch 26/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 1.1613e-04 - val_loss: 1.1505e-04\n",
      "Epoch 27/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.1238e-04 - val_loss: 1.1109e-04\n",
      "Epoch 28/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.0870e-04 - val_loss: 1.0813e-04\n",
      "Epoch 29/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.0550e-04 - val_loss: 1.0468e-04\n",
      "Epoch 30/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 1.0234e-04 - val_loss: 1.0177e-04\n",
      "Epoch 31/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 9.9635e-05 - val_loss: 9.9570e-05\n",
      "Epoch 32/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 9.7523e-05 - val_loss: 9.7549e-05\n",
      "Epoch 33/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 9.5152e-05 - val_loss: 9.6208e-05\n",
      "Epoch 34/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 9.3249e-05 - val_loss: 9.3737e-05\n",
      "Epoch 35/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 9.1451e-05 - val_loss: 9.1563e-05\n",
      "Epoch 36/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 8.9748e-05 - val_loss: 8.9478e-05\n",
      "Epoch 37/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 8.8343e-05 - val_loss: 8.8316e-05\n",
      "Epoch 38/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 8.6855e-05 - val_loss: 8.7246e-05\n",
      "Epoch 39/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 8.6048e-05 - val_loss: 8.5847e-05\n",
      "Epoch 40/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 8.4864e-05 - val_loss: 8.5313e-05\n",
      "Epoch 41/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 8.3844e-05 - val_loss: 8.3946e-05\n",
      "Epoch 42/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 8.2969e-05 - val_loss: 8.3493e-05\n",
      "Epoch 43/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 8.2374e-05 - val_loss: 8.2364e-05\n",
      "Epoch 44/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 8.1574e-05 - val_loss: 8.1536e-05\n",
      "Epoch 45/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 8.1137e-05 - val_loss: 8.0836e-05\n",
      "Epoch 46/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 8.0552e-05 - val_loss: 8.1528e-05\n",
      "Epoch 47/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.9759e-05 - val_loss: 7.9504e-05\n",
      "Epoch 48/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.9401e-05 - val_loss: 7.9836e-05\n",
      "Epoch 49/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.8653e-05 - val_loss: 7.9160e-05\n",
      "Epoch 50/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.8636e-05 - val_loss: 7.8235e-05\n",
      "Epoch 51/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.7926e-05 - val_loss: 7.7564e-05\n",
      "Epoch 52/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.7933e-05 - val_loss: 7.6997e-05\n",
      "Epoch 53/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.7307e-05 - val_loss: 7.6335e-05\n",
      "Epoch 54/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.7039e-05 - val_loss: 7.6389e-05\n",
      "Epoch 55/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.6455e-05 - val_loss: 7.6093e-05\n",
      "Epoch 56/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.6504e-05 - val_loss: 7.5806e-05\n",
      "Epoch 57/1000\n",
      "5706/5706 [==============================] - 0s 10us/step - loss: 7.5988e-05 - val_loss: 7.4976e-05\n",
      "Epoch 58/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 7.5277e-05 - val_loss: 7.4268e-05\n",
      "Epoch 59/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 7.4956e-05 - val_loss: 7.3943e-05\n",
      "Epoch 60/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.4640e-05 - val_loss: 7.4121e-05\n",
      "Epoch 61/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.4024e-05 - val_loss: 7.4258e-05\n",
      "Epoch 62/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.4024e-05 - val_loss: 7.3322e-05\n",
      "Epoch 63/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.3639e-05 - val_loss: 7.3749e-05\n",
      "Epoch 64/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.3257e-05 - val_loss: 7.2088e-05\n",
      "Epoch 65/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.2968e-05 - val_loss: 7.2212e-05\n",
      "Epoch 66/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.2751e-05 - val_loss: 7.1543e-05\n",
      "Epoch 67/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.2156e-05 - val_loss: 7.2345e-05\n",
      "Epoch 68/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 7.1964e-05 - val_loss: 7.0963e-05\n",
      "Epoch 69/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.1706e-05 - val_loss: 7.0405e-05\n",
      "Epoch 70/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.1391e-05 - val_loss: 6.9422e-05\n",
      "Epoch 71/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.0970e-05 - val_loss: 6.9488e-05\n",
      "Epoch 72/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.0344e-05 - val_loss: 7.0533e-05\n",
      "Epoch 73/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.0751e-05 - val_loss: 6.9032e-05\n",
      "Epoch 74/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 7.0348e-05 - val_loss: 6.9601e-05\n",
      "Epoch 75/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.9752e-05 - val_loss: 6.8222e-05\n",
      "Epoch 76/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.9122e-05 - val_loss: 6.8029e-05\n",
      "Epoch 77/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.9142e-05 - val_loss: 6.7343e-05\n",
      "Epoch 78/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.8488e-05 - val_loss: 6.7291e-05\n",
      "Epoch 79/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.8080e-05 - val_loss: 6.7003e-05\n",
      "Epoch 80/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.8231e-05 - val_loss: 6.6655e-05\n",
      "Epoch 81/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.8496e-05 - val_loss: 6.6302e-05\n",
      "Epoch 82/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.7605e-05 - val_loss: 6.6711e-05\n",
      "Epoch 83/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.7571e-05 - val_loss: 6.5848e-05\n",
      "Epoch 84/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.7373e-05 - val_loss: 6.4785e-05\n",
      "Epoch 85/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.6812e-05 - val_loss: 6.4407e-05\n",
      "Epoch 86/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.6618e-05 - val_loss: 6.6893e-05\n",
      "Epoch 87/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.6102e-05 - val_loss: 6.4594e-05\n",
      "Epoch 88/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.5835e-05 - val_loss: 6.3852e-05\n",
      "Epoch 89/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.5455e-05 - val_loss: 6.3525e-05\n",
      "Epoch 90/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.5077e-05 - val_loss: 6.2760e-05\n",
      "Epoch 91/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.5156e-05 - val_loss: 6.2775e-05\n",
      "Epoch 92/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.4339e-05 - val_loss: 6.2099e-05\n",
      "Epoch 93/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.4623e-05 - val_loss: 6.4663e-05\n",
      "Epoch 94/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.4619e-05 - val_loss: 6.2626e-05\n",
      "Epoch 95/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.3985e-05 - val_loss: 6.2147e-05\n",
      "Epoch 96/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.3607e-05 - val_loss: 6.2461e-05\n",
      "Epoch 97/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.3961e-05 - val_loss: 6.2835e-05\n",
      "Epoch 98/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.3331e-05 - val_loss: 6.1246e-05\n",
      "Epoch 99/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.2971e-05 - val_loss: 6.2280e-05\n",
      "Epoch 100/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.2808e-05 - val_loss: 6.1408e-05\n",
      "Epoch 101/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.2657e-05 - val_loss: 6.0138e-05\n",
      "Epoch 102/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.2416e-05 - val_loss: 6.1571e-05\n",
      "Epoch 103/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.2622e-05 - val_loss: 5.9464e-05\n",
      "Epoch 104/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.2260e-05 - val_loss: 5.9588e-05\n",
      "Epoch 105/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.2104e-05 - val_loss: 6.0002e-05\n",
      "Epoch 106/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.1295e-05 - val_loss: 5.8882e-05\n",
      "Epoch 107/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.1257e-05 - val_loss: 5.9591e-05\n",
      "Epoch 108/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.1401e-05 - val_loss: 5.8384e-05\n",
      "Epoch 109/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.0707e-05 - val_loss: 5.8357e-05\n",
      "Epoch 110/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.1027e-05 - val_loss: 5.9490e-05\n",
      "Epoch 111/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.0281e-05 - val_loss: 5.8319e-05\n",
      "Epoch 112/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 6.0262e-05 - val_loss: 5.8931e-05\n",
      "Epoch 113/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 6.0163e-05 - val_loss: 5.8305e-05\n",
      "Epoch 114/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.9881e-05 - val_loss: 5.7615e-05\n",
      "Epoch 115/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 6.0518e-05 - val_loss: 5.7191e-05\n",
      "Epoch 116/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9698e-05 - val_loss: 5.6800e-05\n",
      "Epoch 117/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9176e-05 - val_loss: 5.7702e-05\n",
      "Epoch 118/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9187e-05 - val_loss: 5.6860e-05\n",
      "Epoch 119/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.9254e-05 - val_loss: 5.6814e-05\n",
      "Epoch 120/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9225e-05 - val_loss: 5.8859e-05\n",
      "Epoch 121/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.9405e-05 - val_loss: 5.7417e-05\n",
      "Epoch 122/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8818e-05 - val_loss: 5.7896e-05\n",
      "Epoch 123/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8587e-05 - val_loss: 5.6518e-05\n",
      "Epoch 124/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.8656e-05 - val_loss: 5.6934e-05\n",
      "Epoch 125/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.8058e-05 - val_loss: 5.6089e-05\n",
      "Epoch 126/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.8863e-05 - val_loss: 5.6803e-05\n",
      "Epoch 127/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8741e-05 - val_loss: 5.5852e-05\n",
      "Epoch 128/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8068e-05 - val_loss: 5.6936e-05\n",
      "Epoch 129/1000\n",
      "5706/5706 [==============================] - 0s 13us/step - loss: 5.8127e-05 - val_loss: 5.5074e-05\n",
      "Epoch 130/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8333e-05 - val_loss: 5.7582e-05\n",
      "Epoch 131/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8362e-05 - val_loss: 5.5709e-05\n",
      "Epoch 132/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8202e-05 - val_loss: 5.5367e-05\n",
      "Epoch 133/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7982e-05 - val_loss: 5.5726e-05\n",
      "Epoch 134/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8097e-05 - val_loss: 5.6602e-05\n",
      "Epoch 135/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.9235e-05 - val_loss: 5.6228e-05\n",
      "Epoch 136/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.8538e-05 - val_loss: 5.4630e-05\n",
      "Epoch 137/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7333e-05 - val_loss: 5.4981e-05\n",
      "Epoch 138/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7601e-05 - val_loss: 5.7041e-05\n",
      "Epoch 139/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7830e-05 - val_loss: 5.5369e-05\n",
      "Epoch 140/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7573e-05 - val_loss: 5.5895e-05\n",
      "Epoch 141/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7298e-05 - val_loss: 5.6630e-05\n",
      "Epoch 142/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7589e-05 - val_loss: 5.5459e-05\n",
      "Epoch 143/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7095e-05 - val_loss: 5.6545e-05\n",
      "Epoch 144/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7297e-05 - val_loss: 5.5431e-05\n",
      "Epoch 145/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7265e-05 - val_loss: 5.6213e-05\n",
      "Epoch 146/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7076e-05 - val_loss: 5.4779e-05\n",
      "Epoch 147/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7585e-05 - val_loss: 5.9159e-05\n",
      "Epoch 148/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7466e-05 - val_loss: 5.5014e-05\n",
      "Epoch 149/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7217e-05 - val_loss: 5.6819e-05\n",
      "Epoch 150/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7563e-05 - val_loss: 5.8207e-05\n",
      "Epoch 151/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7121e-05 - val_loss: 5.6053e-05\n",
      "Epoch 152/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7392e-05 - val_loss: 5.7118e-05\n",
      "Epoch 153/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7508e-05 - val_loss: 5.5528e-05\n",
      "Epoch 154/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6808e-05 - val_loss: 5.4151e-05\n",
      "Epoch 155/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7174e-05 - val_loss: 5.4618e-05\n",
      "Epoch 156/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7381e-05 - val_loss: 5.5656e-05\n",
      "Epoch 157/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6993e-05 - val_loss: 5.4843e-05\n",
      "Epoch 158/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6694e-05 - val_loss: 5.8003e-05\n",
      "Epoch 159/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7759e-05 - val_loss: 5.4849e-05\n",
      "Epoch 160/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6453e-05 - val_loss: 5.6107e-05\n",
      "Epoch 161/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6697e-05 - val_loss: 5.5083e-05\n",
      "Epoch 162/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6507e-05 - val_loss: 5.4691e-05\n",
      "Epoch 163/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6564e-05 - val_loss: 5.5004e-05\n",
      "Epoch 164/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6815e-05 - val_loss: 5.6108e-05\n",
      "Epoch 165/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6761e-05 - val_loss: 5.3983e-05\n",
      "Epoch 166/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6293e-05 - val_loss: 5.4424e-05\n",
      "Epoch 167/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6234e-05 - val_loss: 5.4413e-05\n",
      "Epoch 168/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6403e-05 - val_loss: 5.6494e-05\n",
      "Epoch 169/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6855e-05 - val_loss: 5.4092e-05\n",
      "Epoch 170/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6479e-05 - val_loss: 5.4987e-05\n",
      "Epoch 171/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6809e-05 - val_loss: 5.4806e-05\n",
      "Epoch 172/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6776e-05 - val_loss: 5.6226e-05\n",
      "Epoch 173/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7603e-05 - val_loss: 5.4315e-05\n",
      "Epoch 174/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6065e-05 - val_loss: 5.6549e-05\n",
      "Epoch 175/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6176e-05 - val_loss: 5.4082e-05\n",
      "Epoch 176/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6495e-05 - val_loss: 5.5874e-05\n",
      "Epoch 177/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6414e-05 - val_loss: 5.4575e-05\n",
      "Epoch 178/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6379e-05 - val_loss: 5.6381e-05\n",
      "Epoch 179/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.5937e-05 - val_loss: 5.4396e-05\n",
      "Epoch 180/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.7273e-05 - val_loss: 5.5375e-05\n",
      "Epoch 181/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6140e-05 - val_loss: 5.4157e-05\n",
      "Epoch 182/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6262e-05 - val_loss: 5.4303e-05\n",
      "Epoch 183/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6705e-05 - val_loss: 5.6992e-05\n",
      "Epoch 184/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.8113e-05 - val_loss: 5.5180e-05\n",
      "Epoch 185/1000\n",
      "5706/5706 [==============================] - 0s 12us/step - loss: 5.6512e-05 - val_loss: 5.4947e-05\n",
      "Epoch 186/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6550e-05 - val_loss: 5.4873e-05\n",
      "Epoch 187/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6411e-05 - val_loss: 5.4494e-05\n",
      "Epoch 188/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6510e-05 - val_loss: 5.5517e-05\n",
      "Epoch 189/1000\n",
      "5706/5706 [==============================] - 0s 11us/step - loss: 5.6506e-05 - val_loss: 5.4518e-05\n",
      "Epoch 00189: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cafaba8c18>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一對多模型\n",
    "# 因為是一對多模型Timesteps只有1，因此return_sequences=False 才可執行\n",
    "\n",
    "def buildOneToManyModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_length=shape[1], input_dim=shape[2]))\n",
    "    # output shape: (5, 1)\n",
    "    model.add(Dense(1))\n",
    "    model.add(RepeatVector(5)) # 增加維度 if input(None,32) output(None,5,32)\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "# 將pastDay 設為1, futureDay 設為5\n",
    "\n",
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 1, 5)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,:,np.newaxis]\n",
    "Y_val = Y_val[:,:,np.newaxis]\n",
    "\n",
    "model = buildOneToManyModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5706, 1, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14899517,  0.14694595,  0.14490678,  0.1411666 ,  0.15787337,\n",
       "         0.14618045,  0.31321147, -0.04660058, -0.35728018, -0.00508194]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arthur.chang\\AppData\\Local\\Continuum\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \"\"\"\n",
      "C:\\Users\\arthur.chang\\AppData\\Local\\Continuum\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(10, return_sequences=True, input_shape=(5, 10))`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 5, 10)             840       \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 5, 1)              11        \n",
      "=================================================================\n",
      "Total params: 851\n",
      "Trainable params: 851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5703 samples, validate on 633 samples\n",
      "Epoch 1/1000\n",
      "5703/5703 [==============================] - 1s 171us/step - loss: 0.0996 - val_loss: 0.0552\n",
      "Epoch 2/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 0.0360 - val_loss: 0.0170\n",
      "Epoch 3/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 0.0125 - val_loss: 0.0087\n",
      "Epoch 4/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 0.0081 - val_loss: 0.0067\n",
      "Epoch 5/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 0.0065 - val_loss: 0.0056\n",
      "Epoch 6/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 0.0054 - val_loss: 0.0046\n",
      "Epoch 7/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 0.0044 - val_loss: 0.0038\n",
      "Epoch 8/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 0.0036 - val_loss: 0.0030\n",
      "Epoch 9/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 0.0028 - val_loss: 0.0024\n",
      "Epoch 10/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 0.0022 - val_loss: 0.0019\n",
      "Epoch 11/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 12/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 13/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 0.0011 - val_loss: 9.6796e-04\n",
      "Epoch 14/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 8.8366e-04 - val_loss: 8.0059e-04\n",
      "Epoch 15/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 7.3613e-04 - val_loss: 6.7213e-04\n",
      "Epoch 16/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 6.2207e-04 - val_loss: 5.7360e-04\n",
      "Epoch 17/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 5.3305e-04 - val_loss: 4.9622e-04\n",
      "Epoch 18/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 4.6232e-04 - val_loss: 4.3511e-04\n",
      "Epoch 19/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 4.0599e-04 - val_loss: 3.8628e-04\n",
      "Epoch 20/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 3.6134e-04 - val_loss: 3.4708e-04\n",
      "Epoch 21/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 3.2564e-04 - val_loss: 3.1411e-04\n",
      "Epoch 22/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 2.9646e-04 - val_loss: 2.8906e-04\n",
      "Epoch 23/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 2.7273e-04 - val_loss: 2.6707e-04\n",
      "Epoch 24/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 2.5291e-04 - val_loss: 2.4788e-04\n",
      "Epoch 25/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 2.3607e-04 - val_loss: 2.3226e-04\n",
      "Epoch 26/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 2.2226e-04 - val_loss: 2.1864e-04\n",
      "Epoch 27/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 2.0986e-04 - val_loss: 2.0681e-04\n",
      "Epoch 28/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 1.9917e-04 - val_loss: 1.9641e-04\n",
      "Epoch 29/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 1.8959e-04 - val_loss: 1.8743e-04\n",
      "Epoch 30/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 1.8151e-04 - val_loss: 1.7959e-04\n",
      "Epoch 31/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 1.7409e-04 - val_loss: 1.7220e-04\n",
      "Epoch 32/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 1.6751e-04 - val_loss: 1.6669e-04\n",
      "Epoch 33/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 1.6187e-04 - val_loss: 1.6032e-04\n",
      "Epoch 34/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 1.5624e-04 - val_loss: 1.5589e-04\n",
      "Epoch 35/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 1.5203e-04 - val_loss: 1.5100e-04\n",
      "Epoch 36/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 1.4753e-04 - val_loss: 1.4714e-04\n",
      "Epoch 37/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 1.4386e-04 - val_loss: 1.4338e-04\n",
      "Epoch 38/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 1.4060e-04 - val_loss: 1.4165e-04\n",
      "Epoch 39/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 1.3739e-04 - val_loss: 1.3701e-04\n",
      "Epoch 40/1000\n",
      "5703/5703 [==============================] - 0s 41us/step - loss: 1.3420e-04 - val_loss: 1.3451e-04\n",
      "Epoch 41/1000\n",
      "5703/5703 [==============================] - 0s 39us/step - loss: 1.3182e-04 - val_loss: 1.3214e-04\n",
      "Epoch 42/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.2933e-04 - val_loss: 1.2985e-04\n",
      "Epoch 43/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 1.2724e-04 - val_loss: 1.2868e-04\n",
      "Epoch 44/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 1.2539e-04 - val_loss: 1.2779e-04\n",
      "Epoch 45/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 1.2342e-04 - val_loss: 1.2456e-04\n",
      "Epoch 46/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 1.2176e-04 - val_loss: 1.2323e-04\n",
      "Epoch 47/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 1.2081e-04 - val_loss: 1.2135e-04\n",
      "Epoch 48/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 1.1886e-04 - val_loss: 1.2034e-04\n",
      "Epoch 49/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 1.1766e-04 - val_loss: 1.1995e-04\n",
      "Epoch 50/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 1.1634e-04 - val_loss: 1.1867e-04\n",
      "Epoch 51/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 1.1493e-04 - val_loss: 1.1807e-04\n",
      "Epoch 52/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 1.1399e-04 - val_loss: 1.1685e-04\n",
      "Epoch 53/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 1.1330e-04 - val_loss: 1.1521e-04\n",
      "Epoch 54/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 1.1182e-04 - val_loss: 1.1529e-04\n",
      "Epoch 55/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 1.1104e-04 - val_loss: 1.1428e-04\n",
      "Epoch 56/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 1.1027e-04 - val_loss: 1.1305e-04\n",
      "Epoch 57/1000\n",
      "5703/5703 [==============================] - 0s 35us/step - loss: 1.0944e-04 - val_loss: 1.1544e-04\n",
      "Epoch 58/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 1.0846e-04 - val_loss: 1.1151e-04\n",
      "Epoch 59/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 1.0760e-04 - val_loss: 1.1052e-04\n",
      "Epoch 60/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.0818e-04 - val_loss: 1.1124e-04\n",
      "Epoch 61/1000\n",
      "5703/5703 [==============================] - 0s 36us/step - loss: 1.0678e-04 - val_loss: 1.1081e-04\n",
      "Epoch 62/1000\n",
      "5703/5703 [==============================] - 0s 34us/step - loss: 1.0540e-04 - val_loss: 1.0992e-04\n",
      "Epoch 63/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 1.0483e-04 - val_loss: 1.1015e-04\n",
      "Epoch 64/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 1.0407e-04 - val_loss: 1.0806e-04\n",
      "Epoch 65/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 1.0356e-04 - val_loss: 1.0792e-04\n",
      "Epoch 66/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 1.0316e-04 - val_loss: 1.0719e-04\n",
      "Epoch 67/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 1.0252e-04 - val_loss: 1.0895e-04\n",
      "Epoch 68/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5703/5703 [==============================] - 0s 29us/step - loss: 1.0273e-04 - val_loss: 1.0650e-04\n",
      "Epoch 69/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 1.0097e-04 - val_loss: 1.0766e-04\n",
      "Epoch 70/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 1.0072e-04 - val_loss: 1.0642e-04\n",
      "Epoch 71/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 1.0009e-04 - val_loss: 1.0518e-04\n",
      "Epoch 72/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.9820e-05 - val_loss: 1.0404e-04\n",
      "Epoch 73/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.9366e-05 - val_loss: 1.0541e-04\n",
      "Epoch 74/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 9.8850e-05 - val_loss: 1.0458e-04\n",
      "Epoch 75/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.8258e-05 - val_loss: 1.0311e-04\n",
      "Epoch 76/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 9.7876e-05 - val_loss: 1.0378e-04\n",
      "Epoch 77/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.7251e-05 - val_loss: 1.0313e-04\n",
      "Epoch 78/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.6730e-05 - val_loss: 1.0199e-04\n",
      "Epoch 79/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.6909e-05 - val_loss: 1.0505e-04\n",
      "Epoch 80/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 9.6287e-05 - val_loss: 1.0174e-04\n",
      "Epoch 81/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 9.6199e-05 - val_loss: 1.0153e-04\n",
      "Epoch 82/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 9.5169e-05 - val_loss: 1.0146e-04\n",
      "Epoch 83/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 9.5306e-05 - val_loss: 1.0239e-04\n",
      "Epoch 84/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 9.5065e-05 - val_loss: 1.0130e-04\n",
      "Epoch 85/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 9.4925e-05 - val_loss: 1.0044e-04\n",
      "Epoch 86/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 9.4285e-05 - val_loss: 1.0066e-04\n",
      "Epoch 87/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 9.3931e-05 - val_loss: 1.0075e-04\n",
      "Epoch 88/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 9.3368e-05 - val_loss: 1.0032e-04\n",
      "Epoch 89/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.3059e-05 - val_loss: 9.9861e-05\n",
      "Epoch 90/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 9.3178e-05 - val_loss: 1.0336e-04\n",
      "Epoch 91/1000\n",
      "5703/5703 [==============================] - 0s 25us/step - loss: 9.2609e-05 - val_loss: 9.9373e-05\n",
      "Epoch 92/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 9.2330e-05 - val_loss: 9.9392e-05\n",
      "Epoch 93/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 9.2618e-05 - val_loss: 1.0134e-04\n",
      "Epoch 94/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 9.2443e-05 - val_loss: 9.9436e-05\n",
      "Epoch 95/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 9.1405e-05 - val_loss: 9.8906e-05\n",
      "Epoch 96/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 9.1948e-05 - val_loss: 9.8593e-05\n",
      "Epoch 97/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 9.1965e-05 - val_loss: 9.8310e-05\n",
      "Epoch 98/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 9.1357e-05 - val_loss: 9.8948e-05\n",
      "Epoch 99/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 9.1328e-05 - val_loss: 9.8875e-05\n",
      "Epoch 100/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.0855e-05 - val_loss: 9.9827e-05\n",
      "Epoch 101/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.0830e-05 - val_loss: 9.8885e-05\n",
      "Epoch 102/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 9.1315e-05 - val_loss: 9.8385e-05\n",
      "Epoch 103/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 9.0608e-05 - val_loss: 9.8742e-05\n",
      "Epoch 104/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 9.0154e-05 - val_loss: 9.7731e-05\n",
      "Epoch 105/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 9.0262e-05 - val_loss: 1.0001e-04\n",
      "Epoch 106/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 9.0351e-05 - val_loss: 9.8694e-05\n",
      "Epoch 107/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 9.0740e-05 - val_loss: 9.8267e-05\n",
      "Epoch 108/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 9.0706e-05 - val_loss: 1.0030e-04\n",
      "Epoch 109/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 9.1007e-05 - val_loss: 9.7543e-05\n",
      "Epoch 110/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 8.9868e-05 - val_loss: 9.9496e-05\n",
      "Epoch 111/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.1458e-05 - val_loss: 9.8481e-05\n",
      "Epoch 112/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 9.1021e-05 - val_loss: 9.8270e-05\n",
      "Epoch 113/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 8.9835e-05 - val_loss: 9.6467e-05\n",
      "Epoch 114/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 8.9814e-05 - val_loss: 9.7732e-05\n",
      "Epoch 115/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.9999e-05 - val_loss: 1.0094e-04\n",
      "Epoch 116/1000\n",
      "5703/5703 [==============================] - 0s 26us/step - loss: 8.9075e-05 - val_loss: 9.7575e-05\n",
      "Epoch 117/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 8.8837e-05 - val_loss: 9.7011e-05\n",
      "Epoch 118/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 8.9235e-05 - val_loss: 9.7638e-05\n",
      "Epoch 119/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 9.0418e-05 - val_loss: 9.6765e-05\n",
      "Epoch 120/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 8.9656e-05 - val_loss: 9.6553e-05\n",
      "Epoch 121/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.9437e-05 - val_loss: 9.7007e-05\n",
      "Epoch 122/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.9446e-05 - val_loss: 9.6103e-05\n",
      "Epoch 123/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.8777e-05 - val_loss: 1.0011e-04\n",
      "Epoch 124/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.0480e-05 - val_loss: 9.7016e-05\n",
      "Epoch 125/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.9691e-05 - val_loss: 9.8334e-05\n",
      "Epoch 126/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 8.8096e-05 - val_loss: 1.0078e-04\n",
      "Epoch 127/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 8.9112e-05 - val_loss: 9.6647e-05\n",
      "Epoch 128/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 8.9565e-05 - val_loss: 9.7829e-05\n",
      "Epoch 129/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.9718e-05 - val_loss: 9.5804e-05\n",
      "Epoch 130/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 8.8796e-05 - val_loss: 9.8555e-05\n",
      "Epoch 131/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 8.8590e-05 - val_loss: 9.9138e-05\n",
      "Epoch 132/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.8301e-05 - val_loss: 9.5770e-05\n",
      "Epoch 133/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.8756e-05 - val_loss: 9.7486e-05\n",
      "Epoch 134/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.8599e-05 - val_loss: 9.6770e-05\n",
      "Epoch 135/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.9241e-05 - val_loss: 9.6258e-05\n",
      "Epoch 136/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 8.7972e-05 - val_loss: 9.4842e-05\n",
      "Epoch 137/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 8.7969e-05 - val_loss: 9.7577e-05\n",
      "Epoch 138/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 8.7952e-05 - val_loss: 9.6390e-05\n",
      "Epoch 139/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5703/5703 [==============================] - 0s 31us/step - loss: 8.8571e-05 - val_loss: 9.5522e-05\n",
      "Epoch 140/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 8.8952e-05 - val_loss: 9.7100e-05\n",
      "Epoch 141/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 8.7942e-05 - val_loss: 9.5892e-05\n",
      "Epoch 142/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 8.8671e-05 - val_loss: 9.9365e-05\n",
      "Epoch 143/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.7905e-05 - val_loss: 9.6489e-05\n",
      "Epoch 144/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 8.9507e-05 - val_loss: 9.8852e-05\n",
      "Epoch 145/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.8755e-05 - val_loss: 9.5407e-05\n",
      "Epoch 146/1000\n",
      "5703/5703 [==============================] - 0s 28us/step - loss: 8.8501e-05 - val_loss: 9.5517e-05\n",
      "Epoch 147/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 8.9818e-05 - val_loss: 9.8691e-05\n",
      "Epoch 148/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 8.8139e-05 - val_loss: 9.7037e-05\n",
      "Epoch 149/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 9.0668e-05 - val_loss: 9.5215e-05\n",
      "Epoch 150/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 8.7410e-05 - val_loss: 9.5131e-05\n",
      "Epoch 151/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.8177e-05 - val_loss: 9.5944e-05\n",
      "Epoch 152/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 8.8536e-05 - val_loss: 9.4199e-05\n",
      "Epoch 153/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 8.7474e-05 - val_loss: 9.4939e-05\n",
      "Epoch 154/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 8.8637e-05 - val_loss: 9.5698e-05\n",
      "Epoch 155/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 8.7284e-05 - val_loss: 9.4051e-05\n",
      "Epoch 156/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.7412e-05 - val_loss: 9.4440e-05\n",
      "Epoch 157/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 8.7354e-05 - val_loss: 9.8089e-05\n",
      "Epoch 158/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 8.7609e-05 - val_loss: 9.4297e-05\n",
      "Epoch 159/1000\n",
      "5703/5703 [==============================] - 0s 33us/step - loss: 8.6953e-05 - val_loss: 9.5618e-05\n",
      "Epoch 160/1000\n",
      "5703/5703 [==============================] - 0s 31us/step - loss: 8.7964e-05 - val_loss: 9.4883e-05\n",
      "Epoch 161/1000\n",
      "5703/5703 [==============================] - 0s 27us/step - loss: 8.8230e-05 - val_loss: 9.5336e-05\n",
      "Epoch 162/1000\n",
      "5703/5703 [==============================] - 0s 32us/step - loss: 8.8406e-05 - val_loss: 9.4355e-05\n",
      "Epoch 163/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 8.9741e-05 - val_loss: 9.9562e-05\n",
      "Epoch 164/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 8.9850e-05 - val_loss: 9.3214e-05\n",
      "Epoch 165/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 8.8535e-05 - val_loss: 9.7958e-05\n",
      "Epoch 166/1000\n",
      "5703/5703 [==============================] - 0s 30us/step - loss: 8.8444e-05 - val_loss: 9.7005e-05\n",
      "Epoch 167/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.8115e-05 - val_loss: 9.3472e-05\n",
      "Epoch 168/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.7166e-05 - val_loss: 9.3421e-05\n",
      "Epoch 169/1000\n",
      "5703/5703 [==============================] - 0s 29us/step - loss: 8.7139e-05 - val_loss: 9.3745e-05\n",
      "Epoch 00169: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23bee2189b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 多對多模型(輸入與輸出不同長度)\n",
    "# 將return_sequences 設為True ，再用TimeDistributed(Dense(1)) 將輸出調整為(5,1)\n",
    "def buildManyToManyModel(shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, input_length=shape[1], input_dim=shape[2], return_sequences=True))\n",
    "    # output shape: (5, 1)\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "train = readTrain()\n",
    "train_Aug = augFeatures(train)\n",
    "train_norm = normalize(train_Aug)\n",
    "# change the last day and next day \n",
    "X_train, Y_train = buildTrain(train_norm, 5, 5)\n",
    "X_train, Y_train = shuffle(X_train, Y_train)\n",
    "X_train, Y_train, X_val, Y_val = splitData(X_train, Y_train, 0.1)\n",
    "\n",
    "# from 2 dimmension to 3 dimension\n",
    "Y_train = Y_train[:,:,np.newaxis]\n",
    "Y_val = Y_val[:,:,np.newaxis]\n",
    "\n",
    "model = buildManyToManyModel(X_train.shape)\n",
    "callback = EarlyStopping(monitor=\"loss\", patience=10, verbose=1, mode=\"auto\")\n",
    "model.fit(X_train, Y_train, epochs=1000, batch_size=128, validation_data=(X_val, Y_val), callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
